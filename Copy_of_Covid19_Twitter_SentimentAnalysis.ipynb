{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Covid19_Twitter_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/essexgroup31/scrappy-doo/blob/main/Copy_of_Covid19_Twitter_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF2mRiOZa1Ms"
      },
      "source": [
        "# Install Tweepy\n",
        "#!pip install tweepy\n",
        "# Install Matplotlib\n",
        "#!pip install matplotlib\n"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuoQ4ycG3Cf9"
      },
      "source": [
        "londonGeocodething = (51.509865,-0.118092,1000)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBSMP6OHbDo9",
        "outputId": "c42350eb-a586-4f6c-ca1c-013d87970869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "consumer_api_key = 'uQP9N32xpuEIujXsDV2VgcekC'\n",
        "consumer_api_secret = 'aLWDUV2P718ZoOicHCgpvQ8JUWgaOVcce14iA5oXGdtb0PsEV8' \n",
        "access_token = '1315962347836579840-gJhPtLkXpchHCz9gMXzYMl6OX9J8Cs'\n",
        "access_token_sv4ld0F6ecret ='iE0aTCnoKihUv9RkQmAVYYZMeCYh2yT9TprbIG'\n",
        "print (consumer_api_key)\n",
        "print (access_token)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uQP9N32xpuEIujXsDV2VgcekC\n",
            "1315962347836579840-gJhPtLkXpchHCz9gMXzYMl6OX9J8Cs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DXzCMJPbUSe"
      },
      "source": [
        "authorizer = OAuthHandler(consumer_api_key, consumer_api_secret)\n",
        "authorizer.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZkSLu2DfDjN"
      },
      "source": [
        "#search_query = input(\"Hello user, what would you like to search?\\nSearch: \")"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCvHKeQsbfvp"
      },
      "source": [
        "api = tweepy.API(authorizer ,timeout=15)\n",
        "all_tweets = []\n",
        "ukTweetList = []\n",
        "search_query = 'covid19'\n",
        "\n",
        "for tweet_object in tweepy.Cursor(api.search,q=search_query+\" -filter:retweets\",lang='en',result_type='recent').items(2000):\n",
        "    all_tweets.append(tweet_object.text)\n",
        "    #print(tweet_object.user.location)\n",
        "    if (\"UK\" or \"United Kingdom\") in tweet_object.user.location:\n",
        "      ukTweetList.append(tweet_object)\n"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1F1uJGjxe8",
        "outputId": "7447f158-a4de-482b-9cb5-043bc8e0a9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "ukCounties = [\"Bedford\",\"Berk\",\"Bristol\",\"Buckingham\",\"Cambridge\",\"Ches\",\"Cornwall\",\"Cumbria\",\"Derby\",\"Devon\",\"Dorset\",\"Durham\",\"Sussex\",\"Essex\",\"Gloucester\",\"Greater London\", \"Greater Manchester\", \"Hamp\",\"Hereford\",\"Hertford\",\"Isle of Wight\", \"Kent\", \"Lanca\", \"Leicester\",\"Lincoln\",\"City of London\", \"Merseyside\", \"Norfolk\", \"Northampton\", \"Northumberland\", \"North York\", \"Nottingham\", \"Oxford\", \"Rutland\", \"Shrop\", \"Somerset\",\"South York\", \"Staffordshire\", \"Suffold\", \"Surrey\", \"Tyne and Wear\", \"Warwick\", \"West Midlands\", \"West Sussex\", \"West York\", \"Wilt\", \"Worcester\"]\n",
        "# List of uk counties without \"shire\" at the end (because of the way that twitter works with locations)\n",
        "for each in ukTweetList:\n",
        "    location = each.user.location\n",
        "\n",
        "    for each in range(len(location)):\n",
        "      if location[each] == \",\":\n",
        "        if location[each+1] != \" \":\n",
        "          location.replace(\",\",\", \")\n",
        "    \n",
        "    noComma = False\n",
        "    while noComma == False:\n",
        "      if \",\" in location:\n",
        "          noComma = False\n",
        "      else:\n",
        "          noComma = True\n",
        "    \n",
        "      for each in range(len(location)):\n",
        "          if location[each] == \",\":\n",
        "              location = location.replace(\",\",\"\")\n",
        "              break \n",
        "\n",
        "    print(location)\n",
        "\n"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bristol UK\n",
            "Gloucestershire UK\n",
            "London UK\n",
            "UK\n",
            "Leeds UK\n",
            "Leeds UK\n",
            "UK Europe Global\n",
            "UK\n",
            "Norwich Norfolk UK\n",
            "UK\n",
            "Originally from Nottingham UK\n",
            "Oxford UK\n",
            "Coulsdon Surrey UK\n",
            "UK\n",
            "UK\n",
            "13 locations across the UK\n",
            "YorkshireUK\n",
            "Stockton-on-Tees UK\n",
            "London UK\n",
            "UK \n",
            "UK\n",
            "UK\n",
            "Raleigh NC and London UK\n",
            "UK\n",
            "Newcastle upon Tyne UK\n",
            "Loughborough UK\n",
            "UK\n",
            "London UK\n",
            "BB9 Lancashire UK\n",
            "Finchampstead UK\n",
            "UK\n",
            "South Tyneside UK\n",
            "UK\n",
            "UK\n",
            "West Midlands UK\n",
            "UK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03M6gOZzc7rb"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re  \n",
        "import nltk # an amazing library to play with natural language\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIIrJYhVdD9y"
      },
      "source": [
        "tweets = pd.read_csv(\"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\")\n",
        "print(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUFttnmRdTol"
      },
      "source": [
        "X = tweets.iloc[:, 10].values\n",
        "y = tweets.iloc[:, 1].values\n",
        "\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCbe6x3cqOAR"
      },
      "source": [
        "processed_tweets = []\n",
        " \n",
        "for tweet in range(0, len(X)):  \n",
        "    # Remove all the special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', str(X[tweet]))\n",
        " \n",
        "    # remove all single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        " \n",
        "    # Remove single characters from the start\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        " \n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        " \n",
        "    # Removing prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        " \n",
        "    processed_tweets.append(processed_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvvzPXC7daw9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
        "X = tfidfconverter.fit_transform(processed_tweets).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yReZpg4dftB"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "text_classifier = RandomForestClassifier(n_estimators=100, random_state=1)  \n",
        "text_classifier.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiH3lhymdsYI"
      },
      "source": [
        "negativeCounter = 0\n",
        "neutralCounter = 0\n",
        "positiveCounter = 0\n",
        "for tweet in all_tweets:\n",
        "    # Remove all the special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', tweet)\n",
        " \n",
        "    # remove all single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        " \n",
        "    # Remove single characters from the start\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        " \n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        " \n",
        "    # Removing prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        " \n",
        "    sentiment = text_classifier.predict(tfidfconverter.transform([ processed_tweet]).toarray())\n",
        "    \n",
        "\n",
        "    # We need to create something that counts the different sentiments\n",
        "    # Which will then be the data for the pie chart.\n",
        "\n",
        "    if sentiment == \"positive\":\n",
        "      positiveCounter+=1\n",
        "    elif sentiment == \"negative\":\n",
        "      negativeCounter+=1\n",
        "    elif sentiment == \"neutral\":\n",
        "      neutralCounter+=1\n",
        "   \n",
        "    print(tweet, sentiment)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "#print(all_tweets)\n",
        "print(\"Positive sentiments:\",positiveCounter)\n",
        "print(\"Negative sentiments:\",negativeCounter)\n",
        "print(\"Neutral sentiments:\",neutralCounter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQs47arbKTy"
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "labels = \"Negative\", \"Neutral\", \"Positive\"\n",
        "sizes = [negativeCounter,neutralCounter,positiveCounter]\n",
        "print(sizes)\n",
        "colors = [\"red\",\"yellow\",\"green\"]\n",
        "\n",
        "if sizes[0] > sizes[1] & sizes[0] > sizes[2]:\n",
        "  explode = (0.1,0,0)\n",
        "elif sizes[1] > sizes[0] & sizes[1] > sizes[2]:\n",
        "  explode = (0,0.1,0)\n",
        "else:\n",
        "  explode = (0,0,0.1)\n",
        "\n",
        "pyplot.pie(sizes,explode = explode, labels=labels,colors=colors,autopct=\"%1.1f%%\", shadow=True, startangle=140)\n",
        "pyplot.axis(\"equal\")\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}