{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Covid19_Twitter_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/essexgroup31/scrappy-doo/blob/main/Copy_of_Covid19_Twitter_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF2mRiOZa1Ms"
      },
      "source": [
        "# Install Tweepy\n",
        "#!pip install tweepy\n",
        "# Install Matplotlib\n",
        "#!pip install matplotlib\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7urxl91rz4Ff"
      },
      "source": [
        "def removeSuffix(item,suffix):\n",
        "  if item.endswith(suffix):\n",
        "    return item[:-len(suffix)]\n",
        "  else:\n",
        "    return item[:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IClLK4ht3DbW"
      },
      "source": [
        "import string\n",
        "def punctuationRemover(sentence):\n",
        "    myList = []\n",
        "    for each in range(len(sentence)-1):\n",
        "        if sentence[each] in string.punctuation:\n",
        "            if sentence[each+1] in string.ascii_letters:\n",
        "                myList.append(sentence[each])\n",
        "                myList.append(\" \")\n",
        "            else:\n",
        "                myList.append(sentence[each])\n",
        "        else:\n",
        "            myList.append(sentence[each])   # <------------------ HERE IS THE ERROR\n",
        "                \n",
        "    \n",
        "    for each in range(len(myList)):\n",
        "        if myList[each] in string.punctuation:\n",
        "            myList[each] = \"/\"\n",
        "\n",
        "    sentence = \"\"\n",
        "\n",
        "    for each in myList:\n",
        "        if each != \"/\":\n",
        "          sentence = sentence + str(each)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBbrzco8OZKd"
      },
      "source": [
        "def stripShire(sentence):\n",
        "    sentence = sentence.split()\n",
        "    completedsentence = []\n",
        "    \n",
        "    for each in sentence:\n",
        "        completedsentence.append(removeSuffix(each, \"shire\"))\n",
        "    \n",
        "\n",
        "    return completedsentence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBSMP6OHbDo9",
        "outputId": "5ae53ed5-eb51-459f-9431-ec3bcd947ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "consumer_api_key = 'uQP9N32xpuEIujXsDV2VgcekC'\n",
        "consumer_api_secret = 'aLWDUV2P718ZoOicHCgpvQ8JUWgaOVcce14iA5oXGdtb0PsEV8' \n",
        "access_token = '1315962347836579840-WKXz7fp8UEZAaOtABYv1fIpoZpXX7q'\n",
        "access_token_secret ='mitdeExvQ0SFuPGn3HsUhG0faElwQQPSE7FmhaZBNvh76'\n",
        "print (consumer_api_key)\n",
        "print (access_token)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uQP9N32xpuEIujXsDV2VgcekC\n",
            "1315962347836579840-WKXz7fp8UEZAaOtABYv1fIpoZpXX7q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DXzCMJPbUSe"
      },
      "source": [
        "authorizer = OAuthHandler(consumer_api_key, consumer_api_secret)\n",
        "authorizer.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZkSLu2DfDjN"
      },
      "source": [
        "#search_query = input(\"Hello user, what would you like to search?\\nSearch: \")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCvHKeQsbfvp"
      },
      "source": [
        "api = tweepy.API(authorizer ,timeout=15, wait_on_rate_limit=True)\n",
        "all_tweets = []\n",
        "ukTweetList = []\n",
        "search_query = 'covid19'\n",
        "\n",
        "for tweet_object in tweepy.Cursor(api.search,q=search_query+\" -filter:retweets\",lang='en',result_type='recent').items(200):\n",
        "    all_tweets.append(tweet_object.text)\n",
        "    print(tweet_object.author.location)\n",
        "    if (\"UK\" or \"United Kingdom\") in tweet_object.user.location:\n",
        "      ukTweetList.append(tweet_object)\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1F1uJGjxe8",
        "outputId": "04b20227-61eb-4856-e97b-16f4006b9ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "ukCounties = [\"Bedford\",\"Berk\",\"Bristol\",\"Buckingham\",\"Cambridge\",\"Ches\",\"Cornwall\",\"Cumbria\",\"Derby\",\"Devon\",\"Dorset\",\"Durham\",\"Sussex\",\"Essex\",\"Gloucester\",\"City of London\", \"Manchester\", \"Hamp\",\"Hereford\",\"Hertford\",\"Isle of Wight\", \"Kent\", \"Lanca\", \"Leicester\",\"Lincoln\",\"London\", \"Merseyside\", \"Norfolk\", \"Northampton\", \"Northumberland\", \"North York\", \"Nottingham\", \"Oxford\", \"Rutland\", \"Shrop\", \"Somerset\",\"South York\", \"Stafford\", \"Suffold\", \"Surrey\", \"Tyne and Wear\", \"Warwick\", \"West Midlands\", \"West Sussex\", \"West York\", \"Wilt\", \"Worcester\"]\n",
        "tweetLocationData = []\n",
        "# List of uk counties without \"shire\" at the end (because of the way that twitter works with locations)\n",
        "for each in ukTweetList:\n",
        "    tweetData = each\n",
        "    location = each.user.location\n",
        "    \n",
        "    #time to die :D (kill me)\n",
        "    location = punctuationRemover(location)\n",
        "    location = stripShire(location)\n",
        "\n",
        "    tweetData = \"d\"\n",
        "\n",
        "    for x in location:\n",
        "        if x in ukCounties:\n",
        "          location = x\n",
        "          if len(tweetLocationData) == 0:\n",
        "            tweetLocationData.append([location,tweetData])\n",
        "            \n",
        "          else:\n",
        "            for each in range(len(tweetLocationData)):\n",
        "              if tweetLocationData[each][0] == location:\n",
        "                tweetLocationData[each].append(tweetData)\n",
        "                break\n",
        "              else:\n",
        "                tweetLocationData.append([location, tweetData])\n",
        "                break\n",
        "            \n",
        "\n",
        "print(tweetLocationData)\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "West Midlands, UK\n",
            "['West', 'Midlands', 'U']\n",
            "Worldwide - offices in UK+Aus\n",
            "['Worldwide', 'offices', 'in', 'UK', 'Au']\n",
            "London, UK\n",
            "['London', 'U']\n",
            "Canada | USA | UK\n",
            "['Canada', 'USA', 'U']\n",
            "London, UK\n",
            "['London', 'U']\n",
            "UK\n",
            "['U']\n",
            "[['London', 'd', 'd']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03M6gOZzc7rb"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re  \n",
        "import nltk # an amazing library to play with natural language\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIIrJYhVdD9y"
      },
      "source": [
        "tweets = pd.read_csv(\"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\")\n",
        "print(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUFttnmRdTol"
      },
      "source": [
        "X = tweets.iloc[:, 10].values\n",
        "y = tweets.iloc[:, 1].values\n",
        "\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCbe6x3cqOAR"
      },
      "source": [
        "processed_tweets = []\n",
        " \n",
        "for tweet in range(0, len(X)):  \n",
        "    # Remove all the special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', str(X[tweet]))\n",
        " \n",
        "    # remove all single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        " \n",
        "    # Remove single characters from the start\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        " \n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        " \n",
        "    # Removing prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        " \n",
        "    processed_tweets.append(processed_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvvzPXC7daw9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
        "X = tfidfconverter.fit_transform(processed_tweets).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yReZpg4dftB"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "text_classifier = RandomForestClassifier(n_estimators=100, random_state=1)  \n",
        "text_classifier.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiH3lhymdsYI"
      },
      "source": [
        "negativeCounter = 0\n",
        "neutralCounter = 0\n",
        "positiveCounter = 0\n",
        "for tweet in all_tweets:\n",
        "    # Remove all the special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', tweet)\n",
        " \n",
        "    # remove all single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        " \n",
        "    # Remove single characters from the start\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        " \n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        " \n",
        "    # Removing prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        " \n",
        "    sentiment = text_classifier.predict(tfidfconverter.transform([ processed_tweet]).toarray())\n",
        "    \n",
        "\n",
        "    # We need to create something that counts the different sentiments\n",
        "    # Which will then be the data for the pie chart.\n",
        "\n",
        "    if sentiment == \"positive\":\n",
        "      positiveCounter+=1\n",
        "    elif sentiment == \"negative\":\n",
        "      negativeCounter+=1\n",
        "    elif sentiment == \"neutral\":\n",
        "      neutralCounter+=1\n",
        "   \n",
        "    #print(tweet, sentiment)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "#print(all_tweets)\n",
        "print(\"Positive sentiments:\",positiveCounter)\n",
        "print(\"Negative sentiments:\",negativeCounter)\n",
        "print(\"Neutral sentiments:\",neutralCounter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQs47arbKTy"
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "labels = \"Negative\", \"Neutral\", \"Positive\"\n",
        "sizes = [negativeCounter,neutralCounter,positiveCounter]\n",
        "print(sizes)\n",
        "colors = [\"red\",\"yellow\",\"green\"]\n",
        "\n",
        "if sizes[0] > sizes[1] and sizes[0] > sizes[1]:\n",
        "    explode = (0.1,0,0)\n",
        "elif sizes[1] > sizes[0] and sizes[1] > sizes[2]:\n",
        "    explode = (0,0.1,0)\n",
        "else:\n",
        "    explode = (0,0,0.1)\n",
        "\n",
        "\n",
        "pyplot.pie(sizes,explode = explode, labels=labels,colors=colors,autopct=\"%1.1f%%\", shadow=True, startangle=140)\n",
        "pyplot.axis(\"equal\")\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}