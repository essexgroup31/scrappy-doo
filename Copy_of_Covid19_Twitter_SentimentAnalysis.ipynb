{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Covid19_Twitter_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/essexgroup31/scrappy-doo/blob/main/Copy_of_Covid19_Twitter_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF2mRiOZa1Ms"
      },
      "source": [
        "# Install Tweepy\n",
        "#!pip install tweepy\n",
        "# Install Matplotlib\n",
        "#!pip install matplotlib\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7urxl91rz4Ff"
      },
      "source": [
        "def removeSuffix(item,suffix):\n",
        "  if item.endswith(suffix):\n",
        "    return item[:-len(suffix)]\n",
        "  else:\n",
        "    return item[:]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IClLK4ht3DbW"
      },
      "source": [
        "import string\n",
        "def punctuationRemover(sentence):\n",
        "    myList = []\n",
        "    for each in range(len(sentence)-1):\n",
        "        if sentence[each] in string.punctuation:\n",
        "            \n",
        "            if sentence[each+1] in string.ascii_letters:\n",
        "                myList.append(sentence[each])\n",
        "                myList.append(\" \")\n",
        "            else:\n",
        "                myList.append(sentence[each])\n",
        "        else:\n",
        "            myList.append(sentence[each])   # <------------------ HERE IS THE ERROR\n",
        "                \n",
        "    \n",
        "    for each in range(len(myList)):\n",
        "        if myList[each] in string.punctuation:\n",
        "            myList[each] = \"/\"\n",
        "\n",
        "    sentence = \"\"\n",
        "\n",
        "    for each in myList:\n",
        "        if each != \"/\":\n",
        "          sentence = sentence + str(each)\n",
        "    return sentence"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBbrzco8OZKd"
      },
      "source": [
        "def stripShire(sentence):\n",
        "    sentence = sentence.split()\n",
        "    completedsentence = []\n",
        "    \n",
        "    for each in sentence:\n",
        "        completedsentence.append(removeSuffix(each, \"shire\"))\n",
        "    \n",
        "\n",
        "    return completedsentence"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBSMP6OHbDo9",
        "outputId": "5883c20d-a387-4f4e-868c-c9888e8335df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "consumer_api_key = 'uQP9N32xpuEIujXsDV2VgcekC'\n",
        "consumer_api_secret = 'aLWDUV2P718ZoOicHCgpvQ8JUWgaOVcce14iA5oXGdtb0PsEV8' \n",
        "access_token = '1315962347836579840-WKXz7fp8UEZAaOtABYv1fIpoZpXX7q'\n",
        "access_token_secret ='mitdeExvQ0SFuPGn3HsUhG0faElwQQPSE7FmhaZBNvh76'\n",
        "print (consumer_api_key)\n",
        "print (access_token)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uQP9N32xpuEIujXsDV2VgcekC\n",
            "1315962347836579840-WKXz7fp8UEZAaOtABYv1fIpoZpXX7q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DXzCMJPbUSe"
      },
      "source": [
        "authorizer = OAuthHandler(consumer_api_key, consumer_api_secret)\n",
        "authorizer.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZkSLu2DfDjN"
      },
      "source": [
        "#search_query = input(\"Hello user, what would you like to search?\\nSearch: \")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCvHKeQsbfvp"
      },
      "source": [
        "api = tweepy.API(authorizer ,timeout=15, wait_on_rate_limit=True)\n",
        "all_tweets = []\n",
        "\n",
        "search_query = 'covid19'\n",
        "\n",
        "for tweet_object in tweepy.Cursor(api.search,q=search_query+\" -filter:retweets\",lang='en',result_type='recent').items(2000):\n",
        "    all_tweets.append(tweet_object.text)\n",
        "    if (\"UK\" or \"United Kingdom\") in tweet_object.user.location:\n",
        "      ukTweetList.append(tweet_object)\n",
        "    "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1F1uJGjxe8",
        "outputId": "7c2865fe-ff10-407a-ae24-7d1506ea7fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ukCounties = [\"Bedford\",\"Berk\",\"Bristol\",\"Buckingham\",\"Cambridge\",\"Ches\",\"Cornwall\",\"Cumbria\",\"Derby\",\"Devon\",\"Dorset\",\"Durham\",\"Sussex\",\"Essex\",\"Gloucester\",\"City of London\", \"Manchester\", \"Hamp\",\"Hereford\",\"Hertford\",\"Isle of Wight\", \"Kent\", \"Lanca\", \"Leicester\",\"Lincoln\",\"London\", \"Merseyside\", \"Norfolk\", \"Northampton\", \"Northumberland\", \"North York\", \"Nottingham\", \"Oxford\", \"Rutland\", \"Shrop\", \"Somerset\",\"South York\", \"Stafford\", \"Suffold\", \"Surrey\", \"Tyne and Wear\", \"Warwick\", \"West Midlands\", \"West Sussex\", \"West York\", \"Wilt\", \"Worcester\"]\n",
        "countyList = []\n",
        "tweetLocationData = []\n",
        "# List of uk counties without \"shire\" at the end (because of the way that twitter works with locations)\n",
        "for each in ukTweetList:\n",
        "    tweetData = each.text\n",
        "    location = each.user.location\n",
        "    \n",
        "    #time to die :D (kill me)\n",
        "    location = punctuationRemover(location)\n",
        "    location = stripShire(location)\n",
        "\n",
        "    found = False\n",
        "\n",
        "    for x in location:\n",
        "        if x in ukCounties:\n",
        "            location = x\n",
        "            if len(tweetLocationData) == 0:\n",
        "                tweetLocationData.append([location,tweetData])\n",
        "                countyList.append(location)\n",
        "                break\n",
        "            else:\n",
        "                for each in tweetLocationData:\n",
        "                  if each[0] == location:\n",
        "                    found = True\n",
        "                    break\n",
        "                  else:\n",
        "                    found = False\n",
        "                \n",
        "                if found == True:\n",
        "                  each.append(tweetData)\n",
        "                else:\n",
        "                  tweetLocationData.append([location,tweetData])\n",
        "                  countyList.append(location)\n",
        "            \n",
        "\n",
        "print(tweetLocationData)\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['London', 'We continue our #COVID19 myth-busting series with the latest summary of coronavirus news --\\n\\nRead more here ðŸ‘‡\\nhttps://t.co/hAZXrOsm7p', 'Thanks @WFPHA_FMASP for including my photo of #London &amp; @bttowerlondon taken from @uclh during the first wave ofâ€¦ https://t.co/IxW9PmUPmI', 'Researchers argue #COVID19 herd immunity strategy is a \"dangerous fallacy\" in open letter https://t.co/6tCwnXp8t1 https://t.co/zwrSk74w6U', 'ðŸ‘ðŸ’§ Check out Haidar in #Lebanon ðŸ‡±ðŸ‡§ demonstrating how to properly wash your hands. Haidar has been attending virtualâ€¦ https://t.co/XEsuHBWoAA', '#Colombia Remains Attractive Expansion Prospect For Retailers, Despite Covid-19: https://t.co/wQ46o6YmGCâ€¦ https://t.co/KOl2dGOFvd', 'Dr Anna Stewart explains how she and her Clinical Research Unit have adapted to the #COVID19 pandemic.\\n\\nWatch the fâ€¦ https://t.co/akoC7aomff', \"'It is now nine months since I began working with clients in Wuhan who were worried about a rapidly-spreading new vâ€¦ https://t.co/Ax3I7sPsGl\", \"Can a $14bn #infrastructure plan help to kick-start #Mexico's economic recovery from the #Covid19 pandemic? \\n\\nFindâ€¦ https://t.co/N4FbjTfy6K\", \"We're nearly through our special #CCCVideoSeries for the Catholic charities sector. Our next video on #ESG, purposeâ€¦ https://t.co/jXB4uJgDUc\", '@TheLancet There is already sufficient evidence of the disparate effect of COVID19 on the general population. To enâ€¦ https://t.co/ZzCn7l6KHz', '#Europe to #USA 3 months ago: \"You have no clue whatsoever how to deal with this #pandemic.\"\\n#UnitedStates to Europâ€¦ https://t.co/mIN1uIiHH9', '@MeetHajaar @Blecyn1 Real shame tbh! How can they now deploy an unsavored means call \"Covid19\" to deter those proteâ€¦ https://t.co/dBU5cK3ycS', 'Weâ€™re documenting how the humanitarian system is adapting to #COVID19. Explore our new #dataviz to find examples ofâ€¦ https://t.co/l9Y21MKngf', 'Listen to Christina Cassotis, CEO of @PITairport\\ntalk about transforming the airport into a destination and why traâ€¦ https://t.co/PhmJUBngOp', \"Some people's emotional reactions to COVID-19 are thought to be the same reactions they might have to a bereavementâ€¦ https://t.co/8dG5QubL3U\", 'Join @Kafoodle, @QikServe and guest panelist Paul Cowie (Global Head of Catering at OCS Group) to discuss how corpoâ€¦ https://t.co/AbEGDPm7zg', \"Heard the news? We're hosting a session at the #OpenGov #Digital #Youth Summit! How does #COVID19 provide an opportâ€¦ https://t.co/EF8YGRET5w\", 'â€œIt feels like the sacrifices we all made in the initial four or five months were a waste. How long is this going tâ€¦ https://t.co/CqK13Kkak0', 'Listen to @FionaReporter on the one-off #CPhI podcast talking to @Dan5tanton, @BioPlanInc\\nðŸ’‰ How can we produceâ€¦ https://t.co/HuOVNkED3T', 'So, are gyms COVID19 safe? https://t.co/6RQ15CpcJv', '#COVID19 is exposing #health #inequalities &gt; watch this video by the Centre for BME (Black and Minority Ethnic) Heaâ€¦ https://t.co/p2uXCLajTB', 'How do #ambulance services play a pivotal role in responding to #Covid19 ? \\n\\n@AACE_org Chair &amp; @NWAmbulance Chief Eâ€¦ https://t.co/lwApKmk6zT'], ['Kent', 'A new walk-through appointment #Covid19 testing site is open for those with symptoms in the Building 2 car park nexâ€¦ https://t.co/rQq0ioKJEH'], ['Cambridge', 'Work led by @QBI_UCSF found similarities in how 3 coronaviruses, including #SARSCOV2, engage human hosts. Studyingâ€¦ https://t.co/GiFnqkcOQV'], ['Manchester', 'Today we appreciate Dr. Mohammed Kamara, an Emergency Medicine Consultant. We appreciate your time to help educateâ€¦ https://t.co/0WZwPMc3Ez', 'As #Covid19 and #climatechange threaten chaos and racial inequality widens how can the #neweconomy model of #coopsâ€¦ https://t.co/v8l487IqKG'], ['Wilt', 'For \\u2066@rosieposie750\\u2069\\nHelp slow the spread of #COVID19 and identify at risk cases sooner by self-reporting your sympâ€¦ https://t.co/HOXTBRn8GD'], ['Cornwall', \"With public #FireworkDisplays being cancelled due to #Covid19, we're advising against having bonfires &amp; fireworks aâ€¦ https://t.co/TEKOTuRwy2\"], ['Lanca', 'National Grid has published its Winter Outlook report for the coming months, forecasting an 8.3% margin over the coâ€¦ https://t.co/ooEJ9TdPaT'], ['Sussex', 'Chichester City Council launches new Covid19 Civic Awards: In order to acknowledge the efforts of individuals, grouâ€¦ https://t.co/0AJ13UENSm', 'The #KnowledgeShareNHS team is adding new evidence and guidance on #covid19 daily https://t.co/ihqJcL4CpQ https://t.co/N96evb8cQY', 'So, are gyms COVID19 safe? https://t.co/6RQ15CpcJv'], ['Lincoln', \"Remember infectious particles can easily spread 2 metres, especially indoors.\\n\\nLet's keep our social distance and vâ€¦ https://t.co/LMNBZxK1nz\"], ['Oxford', 'Our #faceshields are in #Kenya ðŸ‡°ðŸ‡ª with the wonderful people of @TheNasioTrust. ðŸ™Œ\\U0001f973 ðŸŒ\\n\\nAiming to protect the #medicsâ€¦ https://t.co/4aHLueBfZy'], ['Somerset', 'Thread by @berniespofforthðŸ‘ðŸ‘\\n\"COVID19 has been good for SERCO worldwide with 15,000 staff deployed to maximise profâ€¦ https://t.co/LrSuDDmhG3'], ['Worcester', '.@JulianWTO_UN Will you support the proposal by the governments of India and South Africa to suspend the unjust rulâ€¦ https://t.co/5AL9DBiwJw'], ['Northampton', 'Boris misled #ukparliament. He lied to the queen. He deceived his previous partners and you believe heâ€™s telling yoâ€¦ https://t.co/SkMYTPSd5z']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03M6gOZzc7rb",
        "outputId": "b569a242-cd19-41af-df22-ed4190817d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re  \n",
        "import nltk # an amazing library to play with natural language\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIIrJYhVdD9y",
        "outputId": "e294cdf3-b97f-4eab-dee9-31cbe9670880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "tweets = pd.read_csv(\"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\")\n",
        "print(tweets)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 tweet_id  ...               user_timezone\n",
            "0      570306133677760513  ...  Eastern Time (US & Canada)\n",
            "1      570301130888122368  ...  Pacific Time (US & Canada)\n",
            "2      570301083672813571  ...  Central Time (US & Canada)\n",
            "3      570301031407624196  ...  Pacific Time (US & Canada)\n",
            "4      570300817074462722  ...  Pacific Time (US & Canada)\n",
            "...                   ...  ...                         ...\n",
            "14635  569587686496825344  ...                         NaN\n",
            "14636  569587371693355008  ...                         NaN\n",
            "14637  569587242672398336  ...                         NaN\n",
            "14638  569587188687634433  ...  Eastern Time (US & Canada)\n",
            "14639  569587140490866689  ...                         NaN\n",
            "\n",
            "[14640 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUFttnmRdTol",
        "outputId": "c29e86df-fae0-4ea5-818b-f15c786b9548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "X = tweets.iloc[:, 10].values\n",
        "y = tweets.iloc[:, 1].values\n",
        "\n",
        "print(X)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@VirginAmerica What @dhepburn said.'\n",
            " \"@VirginAmerica plus you've added commercials to the experience... tacky.\"\n",
            " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\"\n",
            " ... '@AmericanAir Please bring American Airlines to #BlackBerry10'\n",
            " \"@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\"\n",
            " '@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCbe6x3cqOAR"
      },
      "source": [
        "processed_tweets = []\n",
        " \n",
        "for tweet in range(0, len(X)):  \n",
        "    # Remove all the special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', str(X[tweet]))\n",
        " \n",
        "    # remove all single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        " \n",
        "    # Remove single characters from the start\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        " \n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        " \n",
        "    # Removing prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        " \n",
        "    processed_tweets.append(processed_tweet)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvvzPXC7daw9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
        "X = tfidfconverter.fit_transform(processed_tweets).toarray()\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yReZpg4dftB",
        "outputId": "f0be2a33-4a15-4d03-ff76-fe5a600206b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "text_classifier = RandomForestClassifier(n_estimators=100, random_state=1)  \n",
        "text_classifier.fit(X, y)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiH3lhymdsYI",
        "outputId": "78bf001e-cc6a-497e-ec4c-c9364f533881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "negativeCounter = 0\n",
        "neutralCounter = 0\n",
        "positiveCounter = 0\n",
        "for tweet in all_tweets:\n",
        "    # Remove all the special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', processed_tweet)\n",
        " \n",
        "    # remove all single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        " \n",
        "    # Remove single characters from the start\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        " \n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        " \n",
        "    # Removing prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        " \n",
        "    sentiment = text_classifier.predict(tfidfconverter.transform([processed_tweet]).toarray())\n",
        "    \n",
        "\n",
        "    # We need to create something that counts the different sentiments\n",
        "    # Which will then be the data for the pie chart.\n",
        "\n",
        "    if sentiment == \"positive\":\n",
        "      positiveCounter+=1\n",
        "    elif sentiment == \"negative\":\n",
        "      negativeCounter+=1\n",
        "    elif sentiment == \"neutral\":\n",
        "      neutralCounter+=1\n",
        "   \n",
        "\n",
        "sentiment_county_data = []\n",
        "found = False\n",
        "\n",
        "for each in range(len(tweetLocationData)):\n",
        "    for x in tweetLocationData[each]:\n",
        "        if x not in countyList:\n",
        "            x = re.sub(r'\\W', ' ', x)\n",
        "            x = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
        "            x = re.sub(r'\\^[a-zA-Z]\\s+', ' ', x)\n",
        "            x = re.sub(r'\\s+', ' ', x, flags=re.I)\n",
        "            x = re.sub(r'^b\\s+', '', x)\n",
        "            x = x.lower()\n",
        "\n",
        "            sentiment = text_classifier.predict(tfidfconverter.transform([x]).toarray())\n",
        "            location = tweetLocationData[each][0]\n",
        "\n",
        "            if len(sentiment_county_data) == 0:\n",
        "                sentiment_county_data.append([location, sentiment])\n",
        "            else:\n",
        "                for each in sentiment_county_data:\n",
        "                    if each[0] == location:\n",
        "                        found = True\n",
        "                        break\n",
        "                    else:\n",
        "                        found = False\n",
        "                if found == True:\n",
        "                    each.append(sentiment)\n",
        "                else:\n",
        "                    sentiment_county_data.append([location,sentiment])\n",
        "\n",
        "print(sentiment_county_data)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "#print(all_tweets)\n",
        "print(\"Positive sentiments:\",positiveCounter)\n",
        "print(\"Negative sentiments:\",negativeCounter)\n",
        "print(\"Neutral sentiments:\",neutralCounter)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b4e1a10d298c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweetLocationData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_county_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQs47arbKTy"
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "labels = \"Negative\", \"Neutral\", \"Positive\"\n",
        "sizes = [negativeCounter,neutralCounter,positiveCounter]\n",
        "print(sizes)\n",
        "colors = [\"red\",\"yellow\",\"green\"]\n",
        "\n",
        "if sizes[0] > sizes[1] and sizes[0] > sizes[1]:\n",
        "    explode = (0.1,0,0)\n",
        "elif sizes[1] > sizes[0] and sizes[1] > sizes[2]:\n",
        "    explode = (0,0.1,0)\n",
        "else:\n",
        "    explode = (0,0,0.1)\n",
        "\n",
        "\n",
        "pyplot.pie(sizes,explode = explode, labels=labels,colors=colors,autopct=\"%1.1f%%\", shadow=True, startangle=140)\n",
        "pyplot.axis(\"equal\")\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}